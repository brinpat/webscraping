{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_headlines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNi/pt0JDSusGlumrM2H/AP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brinpat/webscraping/blob/main/News_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwufXaCZe6Lc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w234H0QvLwk",
        "outputId": "eaee3edd-55a6-4c34-98cf-13cf79974ad5"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 23.1 MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.3.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 758 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.10.8)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=2dd8736fb581ee30b2793e545180d081d1a8c1ff90fa4439c39d0a6d2121e645\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=42f5e245385b76d6a32f4b6731bd589c439b27072c09d2e86d98f122deb153c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=5667cf13fb81866cba98bf7098fd4d67484d898ff7fbc87651eb29271e841fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=723016121a800f80d3b1ed31b7af2007f43ce3c2a6dd379076293c8063369892\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t42CpkEuOa-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7dc187-3108-4345-8683-9117a19ec90b"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from newspaper import Article"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N1jHW55OmWQ"
      },
      "source": [
        "# British news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJFZidBobb-T"
      },
      "source": [
        "url='https://www.bbc.com/news'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYKpHgGOtAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a67f515-5f4e-40f4-e2ba-26b7f48403ca"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3 Accepted-Language: en-US,en;q=0.8'} \n",
        " \n",
        "def weather(city):\n",
        "    city = city.replace(\" \", \"+\")\n",
        "    res = requests.get(\n",
        "        f'https://www.google.com/search?q={city}&oq={city}&aqs=chrome.0.35i39l2j0l4j46j69i60.6128j1j7&sourceid=chrome&ie=UTF-8', headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    location = soup.select('#wob_loc')[0].getText().strip()\n",
        "    time = soup.select('#wob_dts')[0].getText().strip()\n",
        "    info = soup.select('#wob_dc')[0].getText().strip()\n",
        "    weather = soup.select('#wob_tm')[0].getText().strip()\n",
        "    print(location)\n",
        "    print(time)\n",
        "    print(info)\n",
        "    print(weather+\"°C\")\n",
        " \n",
        " \n",
        "city = 'Solihull'\n",
        "city = city+\" weather\"\n",
        "\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "headlines = soup.find('body').find_all('h3')\n",
        "\n",
        "weather(city)\n",
        "print(\"Have a Nice Day :)\"+'\\n')\n",
        "for x in headlines:\n",
        "    print(x.text.strip())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solihull, UK\n",
            "Sunday 9:00 PM\n",
            "Light rain showers\n",
            "59°C\n",
            "Have a Nice Day :)\n",
            "\n",
            "US gunman deliberately sought black victims - mayor\n",
            "US gunman deliberately sought black victims - mayor\n",
            "'Bodies everywhere' - Witnesses tell of Buffalo attack horror\n",
            "Biden: We must work together to address hate\n",
            "Sweden joining Finland in Nato membership bid\n",
            "Russia's invasion not going to plan, Nato says\n",
            "Abortion rights rallies held across US cities\n",
            "Her dad went to prison - so she went to law school\n",
            "Britney Spears announces 'devastating' miscarriage\n",
            "Sweden confirms Nato hopes in historic shift\n",
            "Somalia - where 328 people choose the president\n",
            "Tonga eruption was 'record atmospheric explosion'\n",
            "Somalia - where 328 people choose the president\n",
            "Tonga eruption was 'record atmospheric explosion'\n",
            "Full lunar eclipse to bring super blood Moon\n",
            "Saudi oil giant sees profits jump as prices surge\n",
            "Bumper BBC audience for Eurovision and FA Cup final\n",
            "Fact-checking Russian chemical attack claim\n",
            "Eurovision win brings 'incredible happiness' to Ukraine\n",
            "How do you join Nato?\n",
            "The spy war within the war\n",
            "Where have Ukraine's millions of refugees gone?\n",
            "Fleeing Syria 10 years on: 'We cried all the way'\n",
            "BBC World News TV\n",
            "BBC World Service Radio\n",
            "Eurovision 2022: Highlights of Ukraine's winning night\n",
            "Three invaluable tools to boost your resilience\n",
            "Custom-made brace helps heal baby giraffe\n",
            "Rylan's top tips for Eurovision final\n",
            "Ros Atkins on… The Northern Ireland Protocol\n",
            "N Korea has confirmed Covid: What's likely to happen?\n",
            "‘It feels like being a woman is a crime here’\n",
            "Witness tales shed light on Canada mass shooting\n",
            "‘I searched for my adopted sister for 30 years’\n",
            "Are Australia's refugee releases an election ploy?\n",
            "Search for the lost film that gave birth to Bollywood\n",
            "How a science teacher became a Netflix hitmaker\n",
            "How junk revealed life on Soviet bases\n",
            "The job interviews that go too far\n",
            "Buenos Aires' unusual pizza ritual\n",
            "People who 'danced themselves to death'\n",
            "The UK’s disappearing village\n",
            "The offices turning into luxury flats\n",
            "England's criminal mastermind of £650m\n",
            "News daily newsletter\n",
            "Mobile app\n",
            "Get in touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbSfX8GHJ5Dw"
      },
      "source": [
        "# Indian news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uHqL1MrPx1s",
        "outputId": "70bd6189-3317-4236-cede-ba76b3be0d97"
      },
      "source": [
        "# https://www.geeksforgeeks.org/newspaper-article-scraping-curation-python/\n",
        "#A new article from TOI\n",
        "url = 'http://timesofindia.indiatimes.com/'\n",
        " \n",
        "#For different language newspaper refer above table\n",
        "toi_article = Article(url, language=\"en\") # en for English\n",
        " \n",
        "#To download the article\n",
        "toi_article.download()\n",
        " \n",
        "#To parse the article\n",
        "toi_article.parse()\n",
        " \n",
        "#To perform natural language processing ie..nlp\n",
        "toi_article.nlp()\n",
        " \n",
        "# #To extract title\n",
        "# print(\"Article's Title:\")\n",
        "# print(toi_article.title)\n",
        " \n",
        "#To extract text\n",
        "print(\"Indian news:\")\n",
        "print(toi_article.text)\n",
        " \n",
        "#To extract summary\n",
        "print(\"Article's Summary:\")\n",
        "print(toi_article.summary)\n",
        " \n",
        "#To extract keywords\n",
        "print(\"Article's Keywords:\")\n",
        "print(toi_article.keywords)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian news:\n",
            "Adani to acquire Holcim India assets for $10.5 bn\n",
            "\n",
            "As inflation soars, sale of low-unit price packs jumps\n",
            "\n",
            "Cryptos can lead to dollarisation of economy: RBI officials\n",
            "\n",
            "Elon Musk says Twitter legal team told him he violated an NDA\n",
            "\n",
            "Within hours of curbs, domestic traders claim wheat prices are softening\n",
            "\n",
            "Ambani, UK's tycoon brothers in battle for top drugstore chain\n",
            "Article's Summary:\n",
            "Adani to acquire Holcim India assets for $10.5 bnAs inflation soars, sale of low-unit price packs jumpsCryptos can lead to dollarisation of economy: RBI officialsElon Musk says Twitter legal team told him he violated an NDAWithin hours of curbs, domestic traders claim wheat prices are softeningAmbani, UK's tycoon brothers in battle for top drugstore chain\n",
            "Article's Keywords:\n",
            "['international', 'uks', 'softeningambani', 'india', 'election', 'wheat', 'tycoon', 'traders', 'violated', 'twitter', 'soars', 'business', 'told', 'team']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bZKCu-Vv1Wf"
      },
      "source": [
        "# Chinese news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9nmPQzJv3W-"
      },
      "source": [
        "url = 'https://news.cctv.com/?spm=C73544894212.PKaxMdptdfRq.0.0'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUBZz13PwyUK",
        "outputId": "6d6b308e-3d69-40d0-e8f3-8bfab4049aa7"
      },
      "source": [
        "# https://www.geeksforgeeks.org/newspaper-article-scraping-curation-python/\n",
        "#A new article from TOI\n",
        " \n",
        "#For different language newspaper refer above table\n",
        "toi_article = Article(url, language=\"en\") # en for English\n",
        " \n",
        "#To download the article\n",
        "toi_article.download()\n",
        " \n",
        "#To parse the article\n",
        "toi_article.parse()\n",
        " \n",
        "#To perform natural language processing ie..nlp\n",
        "toi_article.nlp()\n",
        " \n",
        "# #To extract title\n",
        "# print(\"Article's Title:\")\n",
        "# print(toi_article.title)\n",
        " \n",
        "#To extract text\n",
        "print(\"Chinese news:\")\n",
        "print(toi_article.text)\n",
        " \n",
        "#To extract summary\n",
        "print(\"Article's Summary:\")\n",
        "print(toi_article.summary)\n",
        " \n",
        "#To extract keywords\n",
        "print(\"Article's Keywords:\")\n",
        "print(toi_article.keywords)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chinese news:\n",
            "\n",
            "Article's Summary:\n",
            "\n",
            "Article's Keywords:\n",
            "['新闻频道']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Xf5GYwkxcY",
        "outputId": "529ffc97-37fd-4b06-ab24-f3e12adaf577"
      },
      "source": [
        "url = 'https://news.cctv.com/?spm=C73544894212.PKaxMdptdfRq.0.0'\n",
        " \n",
        "# download and parse article\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        " \n",
        "# print article text\n",
        "print(article.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}